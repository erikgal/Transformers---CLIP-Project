{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSvBQoszjYJS","outputId":"bd7cf23c-193d-43ec-b143-48c559a50433"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting timm\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/2.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch\u003e=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n","Collecting huggingface-hub (from timm)\n","  Downloading huggingface_hub-0.16.2-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors (from timm)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.7-\u003etimm) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.7-\u003etimm) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.7-\u003etimm) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.7-\u003etimm) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.7-\u003etimm) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.7-\u003etimm) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.7-\u003etimm) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.7-\u003etimm) (16.0.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003etimm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003etimm) (2.27.1)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003etimm) (4.65.0)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003etimm) (23.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision-\u003etimm) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision-\u003etimm) (8.4.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.7-\u003etimm) (2.1.3)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003etimm) (1.26.16)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003etimm) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003etimm) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003etimm) (3.4)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.7-\u003etimm) (1.3.0)\n","Installing collected packages: safetensors, huggingface-hub, timm\n","Successfully installed huggingface-hub-0.16.2 safetensors-0.3.1 timm-0.9.2\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.2)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers) (4.6.3)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (1.26.16)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.4)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.13.3 transformers-4.30.2\n","Collecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers\u003c5.0.0,\u003e=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.30.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n","Requirement already satisfied: torch\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n","Collecting sentencepiece (from sentence-transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (2.27.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (4.6.3)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.4.0-\u003esentence-transformers) (23.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003esentence-transformers) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003esentence-transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003esentence-transformers) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003esentence-transformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.6.0-\u003esentence-transformers) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.6.0-\u003esentence-transformers) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (0.13.3)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers\u003c5.0.0,\u003e=4.6.0-\u003esentence-transformers) (0.3.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk-\u003esentence-transformers) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk-\u003esentence-transformers) (1.2.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003esentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision-\u003esentence-transformers) (8.4.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.6.0-\u003esentence-transformers) (2.1.3)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (1.26.16)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.4.0-\u003esentence-transformers) (3.4)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.6.0-\u003esentence-transformers) (1.3.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=03454f7e127a168f1dcacf5384120b24dc8a7ba159217bee7bc532c6290a2d61\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence-transformers\n","Installing collected packages: sentencepiece, sentence-transformers\n","Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.56.4)\n","Requirement already satisfied: llvmlite\u003c0.40,\u003e=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.39.1)\n","Requirement already satisfied: numpy\u003c1.24,\u003e=1.18 in /usr/local/lib/python3.10/dist-packages (from numba) (1.22.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba) (67.7.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.3)\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio\u003e=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.56.0)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib\u003c1.1,\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n","Requirement already satisfied: numpy\u003e=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.22.4)\n","Requirement already satisfied: protobuf\u003e=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.1)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.6)\n","Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (5.3.1)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (0.3.0)\n","Requirement already satisfied: six\u003e=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (1.16.0)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib\u003c1.1,\u003e=0.5-\u003etensorboard) (1.3.1)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard) (1.26.16)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard) (3.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug\u003e=1.0.1-\u003etensorboard) (2.1.3)\n","Requirement already satisfied: pyasn1\u003c0.6.0,\u003e=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard) (0.5.0)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c1.1,\u003e=0.5-\u003etensorboard) (3.2.2)\n"]}],"source":["!pip install timm\n","!pip install transformers\n","!pip install sentence-transformers\n","!pip install numba\n","!pip install tensorboard\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/CLIP Project\n","%matplotlib inline\n","%load_ext tensorboard\n","# !tensorboard --logdir=runs --bind_all # http://localhost:6006/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AH9hGZqs9Cy"},"outputs":[],"source":["# !unzip 'Train/resized_train.zip' -d ''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bvXzfQqgi6lT"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-3-944676c57aa4\u003e:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n"]}],"source":["import os\n","import cv2\n","import gc\n","import numpy as np\n","import pandas as pd\n","import itertools\n","from tqdm.autonotebook import tqdm\n","import albumentations as A\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","from torch import nn\n","import torch.nn.functional as F\n","import timm\n","from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n","from sentence_transformers import SentenceTransformer, util\n","# from numba import cuda\n","# device = cuda.get_current_device()\n","# device.reset()\n","%tensorboard --logdir runs"]},{"cell_type":"markdown","metadata":{"id":"o54bL4gji6lU"},"source":["## Config"]},{"cell_type":"markdown","metadata":{"id":"P6En0z5Si6lU"},"source":["*A note on config and CFG: I wrote the codes with python scripts and then converted it into a Jupyter Notebook. So, in case of python scripts, config is a normal python file where I put all the hyperparameters and in the case of Jupyter Notebook, its a class defined in the beginning of the notebook to keep all the hyperparameters.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ulGHg9ai6lV"},"outputs":[],"source":["class CFG:\n","    debug = False\n","    image_path = 'resized_train' #'Train/resized_train'\n","    captions_path = 'Train/caption_prediction_train.csv'\n","    transfer_path = 'Pre-Train/pre_train_100.pt'\n","    batch_size = 64\n","    validation_ratio = 0.2\n","    num_workers = 2\n","    head_lr = 1e-3\n","    image_encoder_lr = 1e-4\n","    text_encoder_lr = 1e-5\n","    logit_scale_lr = 1e-4\n","    weight_decay = 1e-3\n","    patience = 1\n","    factor = 0.8\n","    epochs = 20\n","    cylambda1 = 0.001\n","    cylambda2 = 0.001\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # writer = SummaryWriter(comment=\"convnext_nano\")\n","    writer_comment = '-CyClip_learnable-0.01'\n","\n","    model_name = 'convnext_nano'\n","    m = timm.create_model(model_name, pretrained=True)\n","    input_size = m.pretrained_cfg['input_size']\n","    num_classes = 100 # Number of concepts\n","    image_embedding = m.forward_features(torch.randn(1, input_size[0], input_size[1], input_size[2])).shape[1]\n","    # forward_features(torch.randn(2, 3, 299, 299)).shape[1] # 299, 299\n","    # image_embedding = 112\n","    text_encoder_model = 'distilbert-base-uncased'\n","    text_embedding = 768\n","    text_tokenizer = 'distilbert-base-uncased'\n","    max_length = 200\n","    samples = 10000\n","\n","    pretrained = True # for both image encoder and text encoder\n","    trainable = True # for both image encoder and text encoder\n","    logit_scale_init_value = 0.07 # 2.6592\n","    temperature = 0.07\n","\n","    # image size\n","    size = 224\n","\n","    # for projection head; used for both image and text encoders\n","    num_projection_layers = 1\n","    projection_dim = 256\n","    dropout = 0.1\n","\n","    similarity_threshold = 0.8\n","    global_epoch = 0\n"]},{"cell_type":"markdown","metadata":{"id":"P8OQpI9Xi6lV"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_piLd4dxi6lV"},"outputs":[],"source":["class AvgMeter:\n","    def __init__(self, name=\"Metric\"):\n","        self.name = name\n","        self.reset()\n","\n","    def reset(self):\n","        self.avg_loss, self.accuracy, self.loss_sum, self.correct_predictions, self.count = [0] * 5\n","\n","    def update(self, val, corr, count=1):\n","        self.count += count\n","        self.loss_sum += val * count\n","        self.correct_predictions += corr\n","        self.avg_loss = self.loss_sum / self.count\n","        self.accuracy = self.correct_predictions / self.count\n","\n","    def __repr__(self):\n","        text = f\"{self.name}: {self.avg_loss:.4f}, {self.accuracy:.4f}\"\n","        return text\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group[\"lr\"]\n"]},{"cell_type":"markdown","metadata":{"id":"uM65_Loji6lW"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"-qJIVdNWi6lW"},"source":["As you can see in the tittle image of this article, we need to encode both images and their describing texts. So, the dataset needs to **return both images and texts**. Of course we are not going to feed raw text to our text encoder! We will use **DistilBERT** model (which is smaller than BERT but performs nearly as well as BERT) from **HuggingFace** library as our text encoder; so, we need to **tokenize** the sentences (captions) with DistilBERT tokenizer and then feed the token ids (input_ids) and the attention masks to DistilBERT. Therefore, the dataset needs to take care of the tokenization as well. Below you can see the dataset's code. Below that I'll explain the most important things that is happening in the code."]},{"cell_type":"markdown","metadata":{"id":"UWAVxFMwi6lW"},"source":["In the **\\_\\_init\\_\\_** we receive a tokenizer object which is actually a HuggingFace tokinzer; this tokenizer will be loaded when running the model. We are padding and truncating the captions to a specified max_length. In the **\\_\\_getitem\\_\\_** we will first load an encoded caption which is a dictionary with keys input_ids and attention_mask, make tensors out of its values and after that we will load the corresponding image, transform and augment it (if there is any!) and then we make it a tensor and put it in the dictionary with \"image\" as the key. Finally we put the raw text of the caption with the key \"caption\" in the dictionary only for visualization purposes.\n","\n","I did not use additional data augmentations but you can add them if you want to improve the model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9V91XcNi6lW"},"outputs":[],"source":["class CLIPDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataframe, tokenizer, transforms):\n","        \"\"\"\n","        image_filenames and cpations must have the same length; so, if there are\n","        multiple captions for each image, the image_filenames must have repetitive\n","        file names\n","        \"\"\"\n","\n","        self.image_filenames = dataframe['ID'].values\n","        self.captions = dataframe['caption'].values\n","        self.encoded_captions = tokenizer(\n","            list(self.captions), padding=True, truncation=True, max_length=CFG.max_length\n","        )\n","        self.transforms = transforms\n","\n","    def __getitem__(self, idx):\n","        item = {\n","            key: torch.tensor(values[idx])\n","            for key, values in self.encoded_captions.items()\n","        }\n","\n","        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx] + '.jpg'}\")\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = self.transforms(image=image)['image']\n","        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n","        item['caption'] = self.captions[idx]\n","\n","        return item\n","\n","\n","    def __len__(self):\n","        return len(self.captions)\n","\n","\n","\n","def get_transforms(mode=\"train\"):\n","    if mode == \"train\":\n","        return A.Compose(\n","            [\n","                A.Resize(CFG.size, CFG.size, always_apply=True),\n","                A.Normalize(max_pixel_value=255.0, always_apply=True),\n","            ]\n","        )\n","    else:\n","        return A.Compose(\n","            [\n","                A.Resize(CFG.size, CFG.size, always_apply=True),\n","                A.Normalize(max_pixel_value=255.0, always_apply=True),\n","            ]\n","        )"]},{"cell_type":"markdown","metadata":{"id":"yk394aMmi6lX"},"source":["## Image Encoder"]},{"cell_type":"markdown","metadata":{"id":"3Lnwx026i6lY"},"source":["The image encoder code is straight forward. I'm using PyTorch Image Models library (timm) here which makes a lot of different image models available from ResNets to EfficientNets and many more. Here we will use a ResNet50 as our image encoder. You can easily use torchvision library to use ResNets if you don't want to install a new library."]},{"cell_type":"markdown","metadata":{"id":"xHIYjFc0i6lY"},"source":["The code encodes each image to a fixed size vector with the size of the model's output channels (in case of ResNet50 the vector size will be **2048**). This is the output after the nn.AdaptiveAvgPool2d() layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54yxZ46ulZMt"},"outputs":[],"source":["class TransferImageEncoder(nn.Module):\n","    \"\"\"\n","    Encode images to a fixed size vector\n","    \"\"\"\n","\n","    def __init__(\n","        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n","    ):\n","        super().__init__()\n","        self.model = timm.create_model(\n","            model_name, pretrained, num_classes=CFG.num_classes, global_pool=\"avg\"\n","        )\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0flfMTRi6lY"},"outputs":[],"source":["class ImageEncoder(nn.Module):\n","    \"\"\"\n","    Encode images to a fixed size vector\n","    \"\"\"\n","\n","    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n","        super().__init__()\n","        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool=\"avg\")\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","        self.transfer_model = TransferImageEncoder().to(CFG.device)\n","        self.transfer_model.load_state_dict(torch.load(CFG.transfer_path, map_location=CFG.device))\n","\n","        self.transfer_model.model.head = self.model.head\n","        self.model = self.transfer_model.model\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"markdown","metadata":{"id":"JKRoQ0o9i6lY"},"source":["## Text Encoder"]},{"cell_type":"markdown","metadata":{"id":"-6jTZWtoi6lZ"},"source":["As I mentioned before, I'll use DistilBERT as the text encoder. Like its bigger brother BERT, two special tokens will be added to the actual input tokens: **CLS** and **SEP** which mark the start and end of a sentence. To grab the whole representation of a sentence (as the related BERT and DistilBERT papers point out) we use the final representations of the CLS token and we hope that this representation captures the overall meaning of the sentence (caption). Thinking it in this way, it is similar to what we did to images and converted them into a fixed size vector.\n","\n","In the case of DistilBERT (and also BERT) the output hidden representation for each token is a vector with size **768**. So, the whole caption will be encoded in the CLS token representation whose size is 768."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"am5VR4Ezi6lZ"},"outputs":[],"source":["class TextEncoder(nn.Module):\n","    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n","        super().__init__()\n","        if pretrained:\n","            self.model = DistilBertModel.from_pretrained(model_name)\n","        else:\n","            self.model = DistilBertModel(config=DistilBertConfig())\n","\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","        # we are using the CLS token hidden representation as the sentence's embedding\n","        self.target_token_idx = 0\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden_state = output.last_hidden_state\n","        return last_hidden_state[:, self.target_token_idx, :]"]},{"cell_type":"markdown","metadata":{"id":"tDKLE4cKi6lZ"},"source":["## Projection Head"]},{"cell_type":"markdown","metadata":{"id":"h64obouni6lZ"},"source":["I used [Keras code example implementation](https://keras.io/examples/nlp/nl_image_search/) of projection head to write the following in PyTorch.\n","Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text) we need to bring (project) them into a **new world** (!) with **similar dimensions** for both images and texts in order to be able to compare them and push apart the non-relevant image and texts and pull together those that match. So, the following code will bring the 2048 and 768 dimensional vectors into a 256 (projection_dim) dimensional world, where we can **compare** them.\n","\n","\"embedding_dim\" is the size of the input vector (2048 for images and 768 for texts) and \"projection_dim\" is the the size of the output vector which will be 256 for our case. For understanding the details of this part you can refer to the CLIP paper."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJbY9Yrui6la"},"outputs":[],"source":["class ProjectionHead(nn.Module):\n","    def __init__(\n","        self,\n","        embedding_dim,\n","        projection_dim=CFG.projection_dim,\n","        dropout=CFG.dropout\n","    ):\n","        super().__init__()\n","        self.projection = nn.Linear(embedding_dim, projection_dim)\n","        self.gelu = nn.GELU()\n","        self.fc = nn.Linear(projection_dim, projection_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.layer_norm = nn.LayerNorm(projection_dim)\n","\n","    def forward(self, x):\n","        projected = self.projection(x)\n","        x = self.gelu(projected)\n","        x = self.fc(x)\n","        x = self.dropout(x)\n","        x = x + projected\n","        x = self.layer_norm(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"FUs23eeoi6la"},"source":["## CLIP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MQnmwsWi6lc"},"outputs":[],"source":["class CLIPModel(nn.Module):\n","    def __init__(\n","        self,\n","        temperature=CFG.temperature,\n","        image_embedding=CFG.image_embedding,\n","        text_embedding=CFG.text_embedding,\n","    ):\n","        super().__init__()\n","        self.image_encoder = ImageEncoder()\n","        self.text_encoder = TextEncoder()\n","        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n","        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n","        # self.temperature = torch.nn.Parameter(torch.FloatTensor([temperature]))\n","        self.logit_scale = nn.Parameter(torch.ones([]) * CFG.logit_scale_init_value)\n","\n","    def forward(self, batch):\n","        # Getting Image and Text Features\n","        image_features = self.image_encoder(batch[\"image\"])\n","        text_features = self.text_encoder(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n","        batch_size = batch['input_ids'].shape[0]\n","        temperature = self.logit_scale.exp()\n","\n","        # Getting Image and Text Embeddings (with same dimension)\n","        image_embeddings = self.image_projection(image_features)\n","        text_embeddings = self.text_projection(text_features)\n","        criterion = nn.CrossEntropyLoss(reduction = \"sum\").to(CFG.device)\n","\n","        # Cross-modal contrastive alignment (CLIP)\n","        logits_text_per_image = (image_embeddings @ text_embeddings.T) * temperature\n","        logits_image_per_text = logits_text_per_image.T\n","        target = torch.arange(batch_size).long().to(CFG.device, non_blocking = True)\n","        contrastive_loss = (criterion(logits_text_per_image, target) + criterion(logits_image_per_text, target)) / 2\n","\n","        # In-modal consistency (CyCLIP)\n","        logits_image_per_image = temperature * image_embeddings @ image_embeddings.t()\n","        logits_text_per_text = temperature * text_embeddings @ text_embeddings.t()\n","        inmodal_cyclic_loss = (logits_image_per_image - logits_text_per_text).square().mean() / (temperature * temperature) * batch_size\n","\n","        # Cross-modal consistency (CyCLIP)\n","        crossmodal_cyclic_loss = (logits_text_per_image - logits_image_per_text).square().mean() * (temperature * temperature) * batch_size\n","        crossmodal_cyclic_loss = (logits_text_per_image - logits_image_per_text).square().mean() / (temperature * temperature) * batch_size\n","\n","        cyclic_loss = CFG.cylambda1 * inmodal_cyclic_loss + CFG.cylambda2 * crossmodal_cyclic_loss\n","\n","        loss = contrastive_loss + cyclic_loss\n","\n","        correct_preds = num_of_correct_preds(logits_image_per_text, batch['caption'])\n","\n","        return loss, correct_preds\n","\n","\n","def num_of_correct_preds(logits, captions):\n","    model = SentenceTransformer('bert-base-nli-mean-tokens')\n","    softmax_logits = F.softmax(logits, dim=-1)\n","    correct_predictions = 0\n","    for i, row in enumerate(softmax_logits):\n","        pred = torch.argmax(row)\n","        sentences = [captions[pred].split('\\\"')[1], captions[i].split('\\\"')[1]]\n","        sentence_embeddings = model.encode(sentences)\n","        similarity = F.cosine_similarity(torch.from_numpy(sentence_embeddings[0]), torch.from_numpy(sentence_embeddings[1]), dim=0)\n","        if similarity \u003e= CFG.similarity_threshold:\n","          correct_predictions += 1\n","\n","    return correct_predictions\n"]},{"cell_type":"markdown","metadata":{"id":"TYwBToZFi6ld"},"source":["Now that we've got our targets matrix, we will use simple cross entropy to calculate the actual loss. I've written the full matrix form of cross entropy as a function which you can see in the bottom of the code block. Okay! We are done! Wasn't it simple?! Alright, you can ignore the next paragraph but if you are curious, there is an important note in that."]},{"cell_type":"markdown","metadata":{"id":"cqowBpNXi6ld"},"source":["**Here's why I didn't use a simpler approach**: I need to admit that there's a simpler way to calculate this loss in PyTorch; by doing this: nn.CrossEntropyLoss()(logits, torch.arange(batch_size)). Why I did not use it here? For 2 reasons. 1- The dataset we are using has multiple captions for a single image; so, there is the possibility that two identical images with their similar captions exist in a batch (it is rare but it can happen). Taking the loss with this easier method will ignore this possibility and the model learns to pull apart two representations (assume them different)  that are actually the same. Obviously, we don't want this to happen so I calculated the whole target matrix in a way that takes care of these edge cases. 2- Doing it the way I did, gave me a better understanding of what is happening in this loss function; so, I thought it would give you a better intuition as well!"]},{"cell_type":"markdown","metadata":{"id":"JXkzSurfi6ld"},"source":["## Train"]},{"cell_type":"markdown","metadata":{"id":"BYrHf4yQi6ld"},"source":["Here are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There's not much going on here; just simple training loop and utility functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AX22G56jkUp"},"outputs":[],"source":["def pre_process_data(df):\n","  caption_max = 100\n","  return df[df['caption'].apply(lambda x: len(x.split()) \u003c= caption_max)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfvTzJRqgIxU"},"outputs":[],"source":["def add_concept_to_captions(df_train_captions):\n","  df_train_concepts = pd.read_csv('Train/concept_detection_train.csv', sep='\\t')\n","  df_concepts = pd.read_csv('Train/concepts.csv', sep='\\t')\n","  for train_id, train_caption_row in df_train_captions.iterrows():\n","    img_id = train_caption_row['ID']\n","    cuis = df_train_concepts.loc[df_train_concepts['ID'] == img_id, 'cuis'].values[0].split(';')\n","    caption_builder = \"The concept of this image is \" # sentence_infront_medical_caption\n","\n","\n","    if len(cuis) == 0:\n","        orig_caption = df_train_captions.loc[df_train_captions['ID']==img_id]['caption'].values[0]\n","        df_train_captions.loc[df_train_captions[\"ID\"] == img_id, 'caption'] = \"This image has no concepts. \" + orig_caption\n","        continue\n","\n","    for i, cui in enumerate(cuis):\n","      concept_row = df_concepts.loc[df_concepts['concept'] == cui]['concept_name']\n","      if i==len(cuis)-1 and not i==0:\n","        caption_builder += \" and \" + concept_row.values[0]\n","      elif len(cuis)==1 or i==len(cuis)-2:\n","        caption_builder += concept_row.values[0]\n","      else:\n","        caption_builder += concept_row.values[0] + \", \"\n","\n","    caption_builder += \".\"\n","    orig_caption = df_train_captions.loc[df_train_captions['ID']==img_id]['caption'].values[0]\n","    df_train_captions.loc[df_train_captions[\"ID\"] == img_id, 'caption'] = caption_builder + \" The caption of the medical image is: \\\"\" + orig_caption + \"\\\".\" # sentence_infront_medical_caption\n","\n","\n","\n","  return df_train_captions\n","\n","dataframe = pd.read_csv('Train/caption_prediction_train.csv', sep='\\t')\n","dataframe = dataframe.head(10)\n","dataframe = add_concept_to_captions(dataframe)\n","print(dataframe['caption'][3])\n","print(dataframe['caption'][3].split('\\\"')[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUSC32Cei6ld"},"outputs":[],"source":["def make_train_valid_dfs(add_concepts=False):\n","    dataframe = pd.read_csv('Train/caption_prediction_train.csv', sep='\\t')\n","    # dataframe = dataframe.head(CFG.samples)\n","    dataframe = pre_process_data(dataframe)\n","    dataframe = add_concept_to_captions(dataframe) if add_concepts else dataframe\n","\n","    max_id = dataframe.shape[0]\n","    image_ids = np.arange(0, max_id)\n","    np.random.seed(42)\n","    valid_ids = np.random.choice(\n","        image_ids, size=int(CFG.validation_ratio * len(image_ids)), replace=False\n","    )\n","    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n","    train_dataframe = dataframe.iloc[train_ids,:].reset_index()\n","    valid_dataframe = dataframe.iloc[valid_ids,:].reset_index()\n","    return train_dataframe, valid_dataframe\n","\n","\n","def build_loaders(dataframe, tokenizer, mode):\n","    transforms = get_transforms(mode=mode)\n","    dataset = CLIPDataset(\n","        dataframe=dataframe,\n","        tokenizer=tokenizer,\n","        transforms=transforms,\n","    )\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=CFG.batch_size,\n","        num_workers=CFG.num_workers,\n","        shuffle=True if mode == \"train\" else False,\n","    )\n","    return dataloader"]},{"cell_type":"markdown","metadata":{"id":"i4FcULUYi6le"},"source":["Here's a handy function to train our model. There's not much happening here; just loading the batches, feeding them to the model and stepping the optimizer and lr_scheduler."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jkjp0BEPi6le"},"outputs":[],"source":["def train_epoch(model, train_loader, optimizer, lr_scheduler, step, train_interval, epoch, writer):\n","    metrics = AvgMeter()\n","\n","    tqdm_object = tqdm(train_loader, total=len(train_loader))\n","    for i, batch in enumerate(tqdm_object):\n","        cuda_batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n","        cuda_batch['caption'] = batch['caption']\n","        loss, correct_preds = model(cuda_batch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if step == \"batch\":\n","            lr_scheduler.step()\n","        count = batch[\"image\"].size(0)\n","        metrics.update(loss.item(), correct_preds, count)\n","\n","        if (i + 1) % train_interval == 0 or i + 1 == len(train_loader):\n","            writer.add_scalar(\"Train/Loss\", metrics.avg_loss, len(train_loader)*epoch + i)\n","            writer.add_scalar(\"Train/Accuracy\", metrics.accuracy, len(train_loader)*epoch + i)\n","\n","        tqdm_object.set_postfix(train_loss = metrics.avg_loss, accuracy=metrics.accuracy, lr=get_lr(optimizer))\n","    return metrics\n","\n","\n","def valid_epoch(model, valid_loader, epoch, writer):\n","    metrics = AvgMeter()\n","\n","    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n","    for i, batch in enumerate(tqdm_object):\n","        cuda_batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n","        cuda_batch['caption'] = batch['caption']\n","        loss, correct_preds = model(cuda_batch)\n","\n","        count = batch[\"image\"].size(0)\n","        metrics.update(loss.item(), correct_preds, count)\n","\n","        tqdm_object.set_postfix(valid_loss=metrics.avg_loss, accuracy=metrics.accuracy)\n","\n","    writer.add_scalar(\"Validation/Loss\", metrics.avg_loss, epoch + 1)\n","    writer.add_scalar(\"Validation/Accuracy\", metrics.accuracy, epoch + 1)\n","    return metrics\n","\n","\n","def main():\n","    train_df, valid_df = make_train_valid_dfs(add_concepts=True)\n","    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n","    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n","    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n","\n","    model = CLIPModel().to(CFG.device)\n","    params = [\n","        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n","        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n","        {\"params\": itertools.chain(model.image_projection.parameters(), model.text_projection.parameters()), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay},\n","        {\"params\": model.logit_scale, \"lr\": CFG.logit_scale_lr}\n","    ]\n","    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n","    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n","    )\n","    step = \"epoch\"\n","    train_interval = 25 # Intermediate batch sampling\n","\n","    writer = SummaryWriter(comment=CFG.writer_comment)\n","    best_loss = float('inf')\n","    for epoch in range(CFG.epochs):\n","        print(f\"Epoch: {epoch + 1}, Temperature: {model.logit_scale}\")\n","        print(model.logit_scale.grad)\n","\n","        CFG.global_epoch = epoch + 1\n","        model.train()\n","\n","        train_epoch(model, train_loader, optimizer, lr_scheduler, step, train_interval, epoch, writer)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            valid_metrics = valid_epoch(model, valid_loader, epoch, writer)\n","\n","        if valid_metrics.avg_loss \u003c best_loss:\n","            best_loss = valid_metrics.avg_loss\n","            #torch.save(model.state_dict(), \"new.pt\")\n","            print(\"Saved Best Model!\")\n","\n","        lr_scheduler.step(valid_metrics.avg_loss)\n","\n","    writer.flush()\n","    writer.close()"]},{"cell_type":"markdown","metadata":{"id":"8vBQ2Wuyi6le"},"source":["Running the next cell start training the model. Put the kernel on GPU mode. Every epoch should take about 8 minutes on GPU if you are using 8k version (even one epoch is enough!). It can take some seconds before training actually starts because we are going to encode all the captions once in the train and valid dataset, so please don't stop it! Every thing is working fine."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2G3S841Vi6le"},"outputs":[],"source":["%tensorboard --logdir runs\n","image = None\n","while(image is None):\n","    image = cv2.imread(f\"{CFG.image_path}/ImageCLEFmedCaption_2022_train_053449.jpg\") # FIX colab cv2 error\n","print(image.shape)\n","main()"]},{"cell_type":"markdown","metadata":{"id":"eiMjVEVci6le"},"source":["## Inference"]},{"cell_type":"markdown","metadata":{"id":"WvtxDLeii6lf"},"source":["Okay! We are done with training the model. Now, we need to do inference which in our case will be giving the model a piece of text and want it to retrieve the most relevant images from an unseen validation (or test) set."]},{"cell_type":"markdown","metadata":{"id":"G-dteKJwi6lf"},"source":["### Getting Image Embeddings"]},{"cell_type":"markdown","metadata":{"id":"1JTMqjOwi6lf"},"source":["In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKkDZNP3i6lf"},"outputs":[],"source":["def get_image_embeddings(valid_df, model_path):\n","    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n","    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n","\n","    model = CLIPModel().to(CFG.device)\n","    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n","    model.eval()\n","\n","    valid_image_embeddings = []\n","    with torch.no_grad():\n","        for batch in tqdm(valid_loader):\n","            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n","            image_embeddings = model.image_projection(image_features)\n","            valid_image_embeddings.append(image_embeddings)\n","    return model, torch.cat(valid_image_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySr-oGpMi6lf"},"outputs":[],"source":["image = cv2.imread(f\"{CFG.image_path}/ImageCLEFmedCaption_2022_train_053449.jpg\")\n","_, valid_df = make_train_valid_dfs()\n","model, image_embeddings = get_image_embeddings(valid_df, \"63_percent.pt\")"]},{"cell_type":"markdown","metadata":{"id":"yP81I_wxi6lf"},"source":["### Finding Matches"]},{"cell_type":"markdown","metadata":{"id":"8LF3NabVi6lg"},"source":["This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs after all!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"un6RzF4Ri6lg"},"outputs":[],"source":["def find_matches(model, image_embeddings, query, image_filenames, n=9):\n","    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n","    encoded_query = tokenizer([query])\n","    batch = {\n","        key: torch.tensor(values).to(CFG.device)\n","        for key, values in encoded_query.items()\n","    }\n","    with torch.no_grad():\n","        text_features = model.text_encoder(\n","            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n","        )\n","        text_embeddings = model.text_projection(text_features)\n","\n","    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n","    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n","    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n","\n","    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n","    matches = [image_filenames[idx] for idx in indices[::5]]\n","\n","    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n","    for match, ax in zip(matches, axes.flatten()):\n","        image = cv2.imread(f\"{CFG.image_path}/{match+'.jpg'}\")\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        ax.imshow(image)\n","        ax.axis(\"off\")\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waXVzbioi6lg"},"outputs":[],"source":["find_matches(model,\n","             image_embeddings,\n","             query=\"elbow\",\n","             image_filenames=valid_df['ID'].values,\n","             n=9)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","provenance":[{"file_id":"16_gkO-hCM4qmzi_iQtYL3-LETM7msrQp","timestamp":1676551582768},{"file_id":"1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w","timestamp":1675683105437}],"toc_visible":true,"version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}