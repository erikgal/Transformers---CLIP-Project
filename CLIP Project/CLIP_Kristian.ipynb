{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"g0F0y4dbHX36","executionInfo":{"status":"ok","timestamp":1675256980698,"user_tz":-60,"elapsed":15495,"user":{"displayName":"Kristian Walseth Krøgenes","userId":"05340388489022077074"}}},"outputs":[],"source":["!pip3 install -q transformers datasets # library and function import\n","from google.colab import drive\n","import os\n","import glob\n","from PIL import Image\n","import zipfile\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","import numpy as np\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import TrainingArguments, Trainer\n","from datasets import Dataset, DatasetDict\n","from sklearn.metrics import classification_report, confusion_matrix"]},{"cell_type":"code","source":["drive.mount('/gdrive')\n","%cd /gdrive/My Drive/CLIP Project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9ohPiQuYCNx","executionInfo":{"status":"ok","timestamp":1675256985629,"user_tz":-60,"elapsed":1546,"user":{"displayName":"Kristian Walseth Krøgenes","userId":"05340388489022077074"}},"outputId":"1c05ec82-e1ae-465d-e3b4-1ef510224b61"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/.shortcut-targets-by-id/1Lbh9t5bjB1fmEf1UwwVEMp9R0aW_sIbd/CLIP Project\n"]}]},{"cell_type":"code","source":["# check GPU availability\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if device.type != 'cuda':\n","  raise SystemError('GPU device not found')"],"metadata":{"id":"rKkWYWGXH5cD","executionInfo":{"status":"ok","timestamp":1675256991450,"user_tz":-60,"elapsed":193,"user":{"displayName":"Kristian Walseth Krøgenes","userId":"05340388489022077074"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Import unzipped images\n","# image_list = []\n","# for i, filename in enumerate(glob.glob('Train/resized_train/*.jpg')): #assuming gif\n","#   print(i, filename, type(filename))\n","#   if i==10:\n","#     break\n","  # im=Image.open(filename)\n","  # image_list.append(im)\n","print(len(glob.glob('Train/resized_train/*.jpg')))"],"metadata":{"id":"sLhoMXtMNMZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675257403098,"user_tz":-60,"elapsed":120210,"user":{"displayName":"Kristian Walseth Krøgenes","userId":"05340388489022077074"}},"outputId":"44458f90-ae7e-41a6-8a52-eabcdac0253b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"code","source":["# print(len(image_list))\n","captions = pd.read_csv('Train/caption_prediction_train.csv', header=None, sep='\\t')"],"metadata":{"id":"4zUJ4ejUbyin","executionInfo":{"status":"ok","timestamp":1675249353277,"user_tz":-60,"elapsed":487,"user":{"displayName":"Kristian Walseth Krøgenes","userId":"05340388489022077074"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class ImageCaptioningDataset(Dataset):\n","  def __init__(self, train_captions, root_dir, transform=None, bert_model='distilbert-base-uncased', max_len=512):\n","    self.df = pd.read_csv(train_captions, header=None, sep='\\t')\n","    self.root_dir = root_dir\n","    self.transform = transform\n","    self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","    self.max_len = max_len\n","\n","    self.images = self.df.iloc[1:,0]\n","    self.captions = self.df.iloc[1:,1]\n","\n","  def __len__(self):\n","    return len(self.df)\n","\n","  def generate_img_id(self, id):\n","    return \"0\"*len(str(6-id))+str(id)\n","\n","  def __getitem__(self, idx):\n","    caption = self.captions[idx]\n","    # image = Image.open('Train/resized_train/ImageCLEFmedCaption_2022_train_'+self.generate_img_id(idx)+'.jpg')\n","    image_id = self.images[idx] + '.jpg'\n","    path_to_image = os.path.join(self.root_dir, image_id)\n","    image = Image.open(path_to_image).convert('RGB')\n","    \n","    if self.transform is not None:\n","      image = self.transform(image)\n","\n","    tokenized_caption = self.tokenizer(caption, \n","                                      padding='max_length',  # Pad to max_length\n","                                      truncation=True,  # Truncate to max_length\n","                                      max_length=self.max_len,  \n","                                      return_tensors='pt')['input_ids']\n","    \n","    return image, tokenized_caption"],"metadata":{"id":"41Sfh4vaLds1","executionInfo":{"status":"ok","timestamp":1675252631121,"user_tz":-60,"elapsed":407,"user":{"displayName":"Kristian Walseth Krøgenes","userId":"05340388489022077074"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["root_dir = 'Train/resized_train'\n","train_captions = 'Train/caption_prediction_train.csv'\n","bert_model = 'distilbert-base-uncased'\n","\n","transform = transforms.Compose([transforms.Resize(256),\n","                                transforms.CenterCrop(224),\n","                                transforms.PILToTensor()])\n","\n","train_dataset = ImageCaptioningDataset(train_captions=train_captions,\n","                                       root_dir=root_dir,\n","                                       transform=transform,\n","                                       bert_model=bert_model)\n","train_loader = DataLoader(train_dataset, \n","                          batch_size=64, \n","                          num_workers=2, \n","                          shuffle=True)"],"metadata":{"id":"5Gt_9PGEta0f","executionInfo":{"status":"ok","timestamp":1675252635908,"user_tz":-60,"elapsed":1842,"user":{"displayName":"Kristian Walseth Krøgenes","userId":"05340388489022077074"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["for batch_num, (image, caption) in enumerate(train_loader):\n","  if batch_num > 3:\n","    break\n","  print(f'batch number {batch_num} has {image.shape[0]} images and correspondingly {caption.shape[0]} tokenized captions')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"id":"YUN2darHydXl","executionInfo":{"status":"error","timestamp":1675252640172,"user_tz":-60,"elapsed":432,"user":{"displayName":"Kristian Walseth Krøgenes","userId":"05340388489022077074"}},"outputId":"49198139-5ab1-498e-9cad-8c94af62767d"},"execution_count":70,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-3ba921a79a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'batch number {batch_num} has {image.shape[0]} images and correspondingly {caption.shape[0]} tokenized captions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-68-23233c398d8b>\", line 24, in __getitem__\n    image = Image.open(path_to_image).convert('RGB')\n  File \"/usr/local/lib/python3.8/dist-packages/PIL/Image.py\", line 2843, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: 'Train/resized_train/ImageCLEFmedCaption_2022_train_038572.jpg'\n"]}]}]}